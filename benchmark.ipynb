{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "86ce2593",
   "metadata": {},
   "outputs": [],
   "source": [
    "# execute on Google Colab with T4 GPU\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import time\n",
    "from ncon_torch import ncon\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import scienceplots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "89b27930",
   "metadata": {},
   "outputs": [],
   "source": [
    "def benchmark(n, device, num_instances=10):\n",
    "  num_qubits = n\n",
    "  local_dim = 2\n",
    "\n",
    "  np_times = []\n",
    "  torch_times = []\n",
    "  for _ in range(num_instances): \n",
    "    np_sv = np.random.rand(local_dim**num_qubits) + 1j * np.random.rand(local_dim**num_qubits)\n",
    "    np_sv /= np.linalg.norm(np_sv)\n",
    "    np_sv = np_sv.reshape([local_dim] * num_qubits)\n",
    "    torch_sv = torch.tensor(np_sv, dtype=torch.complex64, device=device)\n",
    "\n",
    "    gate_np = np.random.rand(4, 4) + 1j * np.random.rand(4, 4)\n",
    "    gate_np /= np.linalg.norm(gate_np)\n",
    "    gate_np = gate_np.reshape(2, 2, 2, 2)  # shape: [i', j', i, j] (bra, ket)\n",
    "    gate_torch = torch.tensor(gate_np, dtype=torch.complex64, device=device)\n",
    "\n",
    "    gate_inds = [-1, -2, 1, 2]\n",
    "    state_inds = [1,2] + [-i for i in range(3, num_qubits+1)]\n",
    "\n",
    "    start_np = time.time()\n",
    "    result_np = ncon([gate_np, np_sv], [gate_inds, state_inds])\n",
    "    end_np = time.time()\n",
    "\n",
    "    start_torch = time.time()\n",
    "    result_torch = ncon([gate_torch, torch_sv], [gate_inds, state_inds])\n",
    "    torch.cuda.synchronize()\n",
    "    end_torch = time.time()\n",
    "\n",
    "    np_times.append(end_np-start_np)\n",
    "    torch_times.append(end_torch-start_torch)\n",
    "\n",
    "    assert np.allclose(result_np, result_torch.cpu().numpy()), \"Mismatch between NumPy and PyTorch results!\"\n",
    "  return np.mean(np_times), np.mean(torch_times)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98ddf584",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/12 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "Torch not compiled with CUDA enabled",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 10\u001b[0m\n\u001b[0;32m      8\u001b[0m torch_times \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m n \u001b[38;5;129;01min\u001b[39;00m tqdm(n_range):\n\u001b[1;32m---> 10\u001b[0m     np_time, torch_time \u001b[38;5;241m=\u001b[39m \u001b[43mbenchmark\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     11\u001b[0m     np_times\u001b[38;5;241m.\u001b[39mappend(np_time)\n\u001b[0;32m     12\u001b[0m     torch_times\u001b[38;5;241m.\u001b[39mappend(torch_time)\n",
      "Cell \u001b[1;32mIn[4], line 27\u001b[0m, in \u001b[0;36mbenchmark\u001b[1;34m(n, device, num_instances)\u001b[0m\n\u001b[0;32m     25\u001b[0m start_torch \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m     26\u001b[0m result_torch \u001b[38;5;241m=\u001b[39m ncon([gate_torch, torch_sv], [gate_inds, state_inds])\n\u001b[1;32m---> 27\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcuda\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msynchronize\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     28\u001b[0m end_torch \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m     30\u001b[0m np_times\u001b[38;5;241m.\u001b[39mappend(end_np\u001b[38;5;241m-\u001b[39mstart_np)\n",
      "File \u001b[1;32mc:\\Users\\alamf\\AppData\\Local\\Continuum\\anaconda3\\envs\\qaravan\\Lib\\site-packages\\torch\\cuda\\__init__.py:1038\u001b[0m, in \u001b[0;36msynchronize\u001b[1;34m(device)\u001b[0m\n\u001b[0;32m   1030\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21msynchronize\u001b[39m(device: _device_t \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   1031\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Wait for all kernels in all streams on a CUDA device to complete.\u001b[39;00m\n\u001b[0;32m   1032\u001b[0m \n\u001b[0;32m   1033\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1036\u001b[0m \u001b[38;5;124;03m            if :attr:`device` is ``None`` (default).\u001b[39;00m\n\u001b[0;32m   1037\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 1038\u001b[0m     \u001b[43m_lazy_init\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1039\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mdevice(device):\n\u001b[0;32m   1040\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39m_cuda_synchronize()\n",
      "File \u001b[1;32mc:\\Users\\alamf\\AppData\\Local\\Continuum\\anaconda3\\envs\\qaravan\\Lib\\site-packages\\torch\\cuda\\__init__.py:363\u001b[0m, in \u001b[0;36m_lazy_init\u001b[1;34m()\u001b[0m\n\u001b[0;32m    358\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[0;32m    359\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot re-initialize CUDA in forked subprocess. To use CUDA with \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    360\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmultiprocessing, you must use the \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mspawn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m start method\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    361\u001b[0m     )\n\u001b[0;32m    362\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(torch\u001b[38;5;241m.\u001b[39m_C, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_cuda_getDeviceCount\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m--> 363\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAssertionError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTorch not compiled with CUDA enabled\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    364\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _cudart \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    365\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAssertionError\u001b[39;00m(\n\u001b[0;32m    366\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlibcudart functions unavailable. It looks like you have a broken build?\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    367\u001b[0m     )\n",
      "\u001b[1;31mAssertionError\u001b[0m: Torch not compiled with CUDA enabled"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "\n",
    "n_range = np.arange(4,28,2)\n",
    "np_times = []\n",
    "torch_times = []\n",
    "for n in tqdm(n_range):\n",
    "    np_time, torch_time = benchmark(n, device)\n",
    "    np_times.append(np_time)\n",
    "    torch_times.append(torch_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7c6d168",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_style():\n",
    "    plt.style.use(['science', 'grid'])\n",
    "    plt.rcParams['text.usetex'] = True\n",
    "    plt.rcParams['image.cmap'] = 'cividis'\n",
    "    \n",
    "    plt.rcParams['axes.labelsize'] = 8\n",
    "    plt.rcParams['xtick.labelsize'] = 8  \n",
    "    plt.rcParams['ytick.labelsize'] = 8 \n",
    "    plt.rcParams['legend.fontsize'] = 8\n",
    "    plt.rcParams['axes.titlesize'] = 10\n",
    "    \n",
    "    plt.rcParams['lines.linewidth'] = 1.5\n",
    "    plt.rcParams['savefig.dpi'] = 300\n",
    "    plt.rcParams['figure.dpi'] = 300 \n",
    "    \n",
    "    plt.rcParams['axes.grid'] = True  \n",
    "    plt.rcParams['axes.spines.top'] = True \n",
    "    plt.rcParams['axes.spines.right'] = True\n",
    "    plt.rcParams['xtick.top'] = False  \n",
    "    plt.rcParams['ytick.right'] = False\n",
    "    plt.rcParams['axes.spines.left'] = True\n",
    "    plt.rcParams['ytick.left'] = False\n",
    "\n",
    "set_style()\n",
    "colors = plt.cm.plasma(np.linspace(0.3,0.7,2))\n",
    "fig, ax = plt.subplots(figsize=(3.2,2))\n",
    "plt.plot(n_range, np_times, label='NumPy', marker='o', color=colors[0])\n",
    "plt.plot(n_range, torch_times, label='PyTorch', marker='o', color=colors[1])\n",
    "plt.xlabel('number of qubits')\n",
    "plt.ylabel('time (s)')\n",
    "plt.legend()\n",
    "plt.yscale('log')\n",
    "plt.savefig('benchmark.pdf', bbox_inches='tight')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "qaravan",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
